{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from model import *\n",
    "from prompt import *\n",
    "from batch import *\n",
    "from generate_json import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file and generate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'check_papers.csv'\n",
    "\n",
    "filename = filepath.split('.')[0]\n",
    "df = pd.read_csv(filepath)\n",
    "df['prompt'] = df.apply(create_check_prompt, axis=1)\n",
    "print(len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate chunked jsonl file for batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_in_chunks(df, model_name, custom_id_column, prompt_column, sav_dir, save_filename, chunk_size):\n",
    "    num_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "        save_file = os.path.join(sav_dir, f'{save_filename}_{i+1}.jsonl')\n",
    "        generate_jsonl(chunk, model_name, custom_id_column, prompt_column, save_file)\n",
    "        print(f\"Generated {save_file}\")\n",
    "\n",
    "sav_dir = filename\n",
    "save_filename = filename\n",
    "if not os.path.exists(sav_dir):\n",
    "    os.makedirs(sav_dir, exist_ok=True)\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "custom_id_column = 'id'\n",
    "prompt_column = 'prompt'\n",
    "process_df_in_chunks(df, model_name, custom_id_column, prompt_column, sav_dir, save_filename, chunk_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process batch input to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = get_openai_key(SECRET_FILE)\n",
    "processor = OpenAIBatchProcessor(openai_key)\n",
    "\n",
    "endpoint = \"/v1/chat/completions\"\n",
    "completion_window = \"24h\"\n",
    "\n",
    "read_dir = filename\n",
    "save_dir = filename + '_response'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "read_files = os.listdir(read_dir)\n",
    "read_files = [file for file in read_files if file.endswith('jsonl')]\n",
    "read_files = sorted(read_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "for file in read_files:\n",
    "    read_file = os.path.join(read_dir, file)\n",
    "    save_file = os.path.join(save_dir, file)\n",
    "    results = processor.process_batch(read_file, endpoint, completion_window, save_file)\n",
    "    print(f\"Processed {read_file} and saved to {save_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(save_dir)\n",
    "files = [file for file in files if file.endswith('jsonl')]\n",
    "files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in files:\n",
    "    read_file = os.path.join(save_dir, file)\n",
    "    with open(read_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            try:\n",
    "                custom_id = json_obj['custom_id']\n",
    "                content = json_obj['response']['body']['choices'][0]['message']['content']\n",
    "                data.append({'custom_id': custom_id, 'content': content})\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates(subset=['custom_id'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(filepath)\n",
    "df1 = df1.drop_duplicates(subset=['id'])\n",
    "df2 = df.merge(df1, left_on='custom_id', right_on='id', how='left')\n",
    "df2 = df2.drop(columns=['custom_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = save_dir + '.csv'\n",
    "\n",
    "df2 = df2.rename(columns={'content': 'possible'})\n",
    "cols = df2.columns.tolist()\n",
    "cols.append(cols.pop(cols.index('possible')))\n",
    "df2 = df2[cols]\n",
    "df2.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import *\n",
    "from prompt import *\n",
    "from batch import *\n",
    "from generate_json import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100400\n",
      "97242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>display_name</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>language</th>\n",
       "      <th>type</th>\n",
       "      <th>type_crossref</th>\n",
       "      <th>...</th>\n",
       "      <th>locations.source.type</th>\n",
       "      <th>locations.source</th>\n",
       "      <th>sustainable_development_goals.id</th>\n",
       "      <th>sustainable_development_goals.score</th>\n",
       "      <th>sustainable_development_goals.display_name</th>\n",
       "      <th>grants.funder</th>\n",
       "      <th>grants.funder_display_name</th>\n",
       "      <th>grants.award_id</th>\n",
       "      <th>counts_by_year.year</th>\n",
       "      <th>counts_by_year.cited_by_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31287</th>\n",
       "      <td>https://openalex.org/W2894846460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zooming Network</td>\n",
       "      <td>Zooming Network</td>\n",
       "      <td>2.071526</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>en</td>\n",
       "      <td>preprint</td>\n",
       "      <td>posted-content</td>\n",
       "      <td>...</td>\n",
       "      <td>repository</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://metadata.un.org/sdg/16</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Peace, justice, and strong institutions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>https://openalex.org/W2895320328</td>\n",
       "      <td>https://doi.org/10.1039/c8na00090e</td>\n",
       "      <td>Porous tal palm carbon nanosheets: preparation...</td>\n",
       "      <td>Porous tal palm carbon nanosheets: preparation...</td>\n",
       "      <td>10.670827</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>article</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>...</td>\n",
       "      <td>journal|repository|repository</td>\n",
       "      <td>nan|nan|nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openalex.org/F4320322323|https://opena...</td>\n",
       "      <td>King Fahd University of Petroleum and Minerals...</td>\n",
       "      <td>None|None</td>\n",
       "      <td>2024|2023|2022|2021|2020|2019</td>\n",
       "      <td>8|23|15|21|10|8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46082</th>\n",
       "      <td>https://openalex.org/W2895487477</td>\n",
       "      <td>https://doi.org/10.1063/1.5062649</td>\n",
       "      <td>Overall assessment on flexible pavement mainte...</td>\n",
       "      <td>Overall assessment on flexible pavement mainte...</td>\n",
       "      <td>0.540709</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>article</td>\n",
       "      <td>proceedings-article</td>\n",
       "      <td>...</td>\n",
       "      <td>journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10532</th>\n",
       "      <td>https://openalex.org/W2895623723</td>\n",
       "      <td>https://doi.org/10.1016/j.renene.2018.10.002</td>\n",
       "      <td>Potential use of flocculating oleaginous yeast...</td>\n",
       "      <td>Potential use of flocculating oleaginous yeast...</td>\n",
       "      <td>5.783283</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>en</td>\n",
       "      <td>article</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>...</td>\n",
       "      <td>journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://metadata.un.org/sdg/12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>Responsible consumption and production</td>\n",
       "      <td>https://openalex.org/F4320322614</td>\n",
       "      <td>Thailand Research Fund</td>\n",
       "      <td>RTA 6080010</td>\n",
       "      <td>2023|2022|2021|2020|2019</td>\n",
       "      <td>4|5|6|9|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20957</th>\n",
       "      <td>https://openalex.org/W2894642382</td>\n",
       "      <td>https://doi.org/10.1038/s41598-018-33402-1</td>\n",
       "      <td>Attenuation of hypertension by C-fiber stimula...</td>\n",
       "      <td>Attenuation of hypertension by C-fiber stimula...</td>\n",
       "      <td>3.329368</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>en</td>\n",
       "      <td>article</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>...</td>\n",
       "      <td>journal|repository|repository|repository|journ...</td>\n",
       "      <td>nan|nan|nan|nan|nan|nan|nan|nan</td>\n",
       "      <td>https://metadata.un.org/sdg/3|https://metadata...</td>\n",
       "      <td>0.82|0.82</td>\n",
       "      <td>Good health and well-being|Good health and wel...</td>\n",
       "      <td>https://openalex.org/F4320322113|https://opena...</td>\n",
       "      <td>Korea Institute of Oriental Medicine|National ...</td>\n",
       "      <td>K18181|2017R1E1A2A01079599|K18181|2017R1E1A2A0...</td>\n",
       "      <td>2024|2023|2022|2020|2019|2024|2023|2022|2020|2019</td>\n",
       "      <td>1|5|3|2|1|1|5|3|2|1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "31287  https://openalex.org/W2894846460   \n",
       "5324   https://openalex.org/W2895320328   \n",
       "46082  https://openalex.org/W2895487477   \n",
       "10532  https://openalex.org/W2895623723   \n",
       "20957  https://openalex.org/W2894642382   \n",
       "\n",
       "                                                doi  \\\n",
       "31287                                           NaN   \n",
       "5324             https://doi.org/10.1039/c8na00090e   \n",
       "46082             https://doi.org/10.1063/1.5062649   \n",
       "10532  https://doi.org/10.1016/j.renene.2018.10.002   \n",
       "20957    https://doi.org/10.1038/s41598-018-33402-1   \n",
       "\n",
       "                                                   title  \\\n",
       "31287                                    Zooming Network   \n",
       "5324   Porous tal palm carbon nanosheets: preparation...   \n",
       "46082  Overall assessment on flexible pavement mainte...   \n",
       "10532  Potential use of flocculating oleaginous yeast...   \n",
       "20957  Attenuation of hypertension by C-fiber stimula...   \n",
       "\n",
       "                                            display_name  relevance_score  \\\n",
       "31287                                    Zooming Network         2.071526   \n",
       "5324   Porous tal palm carbon nanosheets: preparation...        10.670827   \n",
       "46082  Overall assessment on flexible pavement mainte...         0.540709   \n",
       "10532  Potential use of flocculating oleaginous yeast...         5.783283   \n",
       "20957  Attenuation of hypertension by C-fiber stimula...         3.329368   \n",
       "\n",
       "       publication_year publication_date language      type  \\\n",
       "31287              2018       2018-10-04       en  preprint   \n",
       "5324               2019       2019-01-01       en   article   \n",
       "46082              2018       2018-01-01       en   article   \n",
       "10532              2019       2019-06-01       en   article   \n",
       "20957              2018       2018-10-08       en   article   \n",
       "\n",
       "             type_crossref  ...  \\\n",
       "31287       posted-content  ...   \n",
       "5324       journal-article  ...   \n",
       "46082  proceedings-article  ...   \n",
       "10532      journal-article  ...   \n",
       "20957      journal-article  ...   \n",
       "\n",
       "                                   locations.source.type  \\\n",
       "31287                                         repository   \n",
       "5324                       journal|repository|repository   \n",
       "46082                                            journal   \n",
       "10532                                            journal   \n",
       "20957  journal|repository|repository|repository|journ...   \n",
       "\n",
       "                      locations.source  \\\n",
       "31287                              NaN   \n",
       "5324                       nan|nan|nan   \n",
       "46082                              NaN   \n",
       "10532                              NaN   \n",
       "20957  nan|nan|nan|nan|nan|nan|nan|nan   \n",
       "\n",
       "                        sustainable_development_goals.id  \\\n",
       "31287                     https://metadata.un.org/sdg/16   \n",
       "5324                                                 NaN   \n",
       "46082                                                NaN   \n",
       "10532                     https://metadata.un.org/sdg/12   \n",
       "20957  https://metadata.un.org/sdg/3|https://metadata...   \n",
       "\n",
       "      sustainable_development_goals.score  \\\n",
       "31287                                0.75   \n",
       "5324                                  NaN   \n",
       "46082                                 NaN   \n",
       "10532                                0.43   \n",
       "20957                           0.82|0.82   \n",
       "\n",
       "              sustainable_development_goals.display_name  \\\n",
       "31287            Peace, justice, and strong institutions   \n",
       "5324                                                 NaN   \n",
       "46082                                                NaN   \n",
       "10532             Responsible consumption and production   \n",
       "20957  Good health and well-being|Good health and wel...   \n",
       "\n",
       "                                           grants.funder  \\\n",
       "31287                                                NaN   \n",
       "5324   https://openalex.org/F4320322323|https://opena...   \n",
       "46082                                                NaN   \n",
       "10532                   https://openalex.org/F4320322614   \n",
       "20957  https://openalex.org/F4320322113|https://opena...   \n",
       "\n",
       "                              grants.funder_display_name  \\\n",
       "31287                                                NaN   \n",
       "5324   King Fahd University of Petroleum and Minerals...   \n",
       "46082                                                NaN   \n",
       "10532                             Thailand Research Fund   \n",
       "20957  Korea Institute of Oriental Medicine|National ...   \n",
       "\n",
       "                                         grants.award_id  \\\n",
       "31287                                                NaN   \n",
       "5324                                           None|None   \n",
       "46082                                                NaN   \n",
       "10532                                        RTA 6080010   \n",
       "20957  K18181|2017R1E1A2A01079599|K18181|2017R1E1A2A0...   \n",
       "\n",
       "                                     counts_by_year.year  \\\n",
       "31287                                               2019   \n",
       "5324                       2024|2023|2022|2021|2020|2019   \n",
       "46082                                                NaN   \n",
       "10532                           2023|2022|2021|2020|2019   \n",
       "20957  2024|2023|2022|2020|2019|2024|2023|2022|2020|2019   \n",
       "\n",
       "       counts_by_year.cited_by_count  \n",
       "31287                              1  \n",
       "5324                 8|23|15|21|10|8  \n",
       "46082                            NaN  \n",
       "10532                      4|5|6|9|1  \n",
       "20957            1|5|3|2|1|1|5|3|2|1  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('works-2018-2022.csv', low_memory=False)\n",
    "data2 = pd.read_csv('works-2023-2024.csv', low_memory=False)\n",
    "df = pd.concat([data1, data2])\n",
    "print(len(df))\n",
    "df['publication_date'] = pd.to_datetime(df['publication_date'])\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "df = df[df['created_date'] >= '2018-10-11']\n",
    "df = df.sort_values(by='created_date')\n",
    "print(len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "# df.set_index('created_date', inplace=True)\n",
    "\n",
    "# # Resample by month and count papers\n",
    "# monthly_papers = df.resample('M').size()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# monthly_papers.plot(kind='line', marker='o')\n",
    "# plt.title('Monthly Change in Number of Papers')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Number of Papers')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "doi\n",
      "title\n",
      "display_name\n",
      "relevance_score\n",
      "publication_year\n",
      "publication_date\n",
      "language\n",
      "type\n",
      "type_crossref\n",
      "indexed_in\n",
      "countries_distinct_count\n",
      "institutions_distinct_count\n",
      "corresponding_author_ids\n",
      "corresponding_institution_ids\n",
      "apc_list\n",
      "apc_paid\n",
      "fwci\n",
      "has_fulltext\n",
      "cited_by_count\n",
      "is_retracted\n",
      "is_paratext\n",
      "locations_count\n",
      "datasets\n",
      "versions\n",
      "referenced_works_count\n",
      "referenced_works\n",
      "related_works\n",
      "ngrams_url\n",
      "cited_by_api_url\n",
      "updated_date\n",
      "created_date\n",
      "ids.openalex\n",
      "ids.doi\n",
      "ids.mag\n",
      "primary_location.is_oa\n",
      "primary_location.landing_page_url\n",
      "primary_location.pdf_url\n",
      "primary_location.source.id\n",
      "primary_location.source.display_name\n",
      "primary_location.source.issn_l\n",
      "primary_location.source.issn\n",
      "primary_location.source.is_oa\n",
      "primary_location.source.is_in_doaj\n",
      "primary_location.source.host_organization\n",
      "primary_location.source.host_organization_name\n",
      "primary_location.source.host_organization_lineage\n",
      "primary_location.source.host_organization_lineage_names\n",
      "primary_location.source.type\n",
      "primary_location.license\n",
      "primary_location.license_id\n",
      "primary_location.version\n",
      "primary_location.is_accepted\n",
      "primary_location.is_published\n",
      "open_access.is_oa\n",
      "open_access.oa_status\n",
      "open_access.oa_url\n",
      "open_access.any_repository_has_fulltext\n",
      "cited_by_percentile_year.min\n",
      "cited_by_percentile_year.max\n",
      "biblio.volume\n",
      "biblio.issue\n",
      "biblio.first_page\n",
      "biblio.last_page\n",
      "primary_topic.id\n",
      "primary_topic.display_name\n",
      "primary_topic.score\n",
      "primary_topic.subfield.id\n",
      "primary_topic.subfield.display_name\n",
      "primary_topic.field.id\n",
      "primary_topic.field.display_name\n",
      "primary_topic.domain.id\n",
      "primary_topic.domain.display_name\n",
      "best_oa_location.is_oa\n",
      "best_oa_location.landing_page_url\n",
      "best_oa_location.pdf_url\n",
      "best_oa_location.source.id\n",
      "best_oa_location.source.display_name\n",
      "best_oa_location.source.issn_l\n",
      "best_oa_location.source.issn\n",
      "best_oa_location.source.is_oa\n",
      "best_oa_location.source.is_in_doaj\n",
      "best_oa_location.source.host_organization\n",
      "best_oa_location.source.host_organization_name\n",
      "best_oa_location.source.host_organization_lineage\n",
      "best_oa_location.source.host_organization_lineage_names\n",
      "best_oa_location.source.type\n",
      "best_oa_location.license\n",
      "best_oa_location.license_id\n",
      "best_oa_location.version\n",
      "best_oa_location.is_accepted\n",
      "best_oa_location.is_published\n",
      "primary_location.source\n",
      "best_oa_location.source\n",
      "fulltext_origin\n",
      "apc_list.value\n",
      "apc_list.currency\n",
      "apc_list.value_usd\n",
      "apc_list.provenance\n",
      "apc_paid.value\n",
      "apc_paid.currency\n",
      "apc_paid.value_usd\n",
      "apc_paid.provenance\n",
      "is_authors_truncated\n",
      "best_oa_location\n",
      "ids.pmid\n",
      "ids.pmcid\n",
      "abstract\n",
      "primary_location\n",
      "primary_topic\n",
      "authorships.author_position\n",
      "authorships.institutions\n",
      "authorships.countries\n",
      "authorships.is_corresponding\n",
      "authorships.raw_author_name\n",
      "authorships.raw_affiliation_strings\n",
      "authorships.affiliations\n",
      "authorships.author.id\n",
      "authorships.author.display_name\n",
      "authorships.author.orcid\n",
      "topics.id\n",
      "topics.display_name\n",
      "topics.score\n",
      "topics.subfield.id\n",
      "topics.subfield.display_name\n",
      "topics.field.id\n",
      "topics.field.display_name\n",
      "topics.domain.id\n",
      "topics.domain.display_name\n",
      "keywords.id\n",
      "keywords.display_name\n",
      "keywords.score\n",
      "concepts.id\n",
      "concepts.wikidata\n",
      "concepts.display_name\n",
      "concepts.level\n",
      "concepts.score\n",
      "mesh.descriptor_ui\n",
      "mesh.descriptor_name\n",
      "mesh.qualifier_ui\n",
      "mesh.qualifier_name\n",
      "mesh.is_major_topic\n",
      "locations.is_oa\n",
      "locations.landing_page_url\n",
      "locations.pdf_url\n",
      "locations.license\n",
      "locations.license_id\n",
      "locations.version\n",
      "locations.is_accepted\n",
      "locations.is_published\n",
      "locations.source.id\n",
      "locations.source.display_name\n",
      "locations.source.issn_l\n",
      "locations.source.issn\n",
      "locations.source.is_oa\n",
      "locations.source.is_in_doaj\n",
      "locations.source.host_organization\n",
      "locations.source.host_organization_name\n",
      "locations.source.host_organization_lineage\n",
      "locations.source.host_organization_lineage_names\n",
      "locations.source.type\n",
      "locations.source\n",
      "sustainable_development_goals.id\n",
      "sustainable_development_goals.score\n",
      "sustainable_development_goals.display_name\n",
      "grants.funder\n",
      "grants.funder_display_name\n",
      "grants.award_id\n",
      "counts_by_year.year\n",
      "counts_by_year.cited_by_count\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perform initial cleaning\n",
    "\n",
    "This part aims to initially address the collected data. \n",
    "\n",
    "- Only consider preprint and article for analysis\n",
    "- Remove duplicates\n",
    "\n",
    "- Whether to remove records without doi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87489\n"
     ]
    }
   ],
   "source": [
    "types = ['preprint', 'article']\n",
    "\n",
    "df1 = df[df['type'].isin(types)]\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/3581942531.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].astype(str)\n",
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/3581942531.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['processed_title'] = df['title'].apply(preprocess_text)\n",
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/3581942531.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_title'] = df['title'].apply(preprocess_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70644\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Remove punctuation and lowercase the title.\"\"\"\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicates and keep the last one.\"\"\"\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['processed_title'] = df['title'].apply(preprocess_text)\n",
    "    df = df.drop_duplicates(subset=['processed_title'], keep='last')\n",
    "    df = df.drop(columns=['processed_title'])\n",
    "    return df\n",
    "\n",
    "df2 = remove_duplicates(df1)\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Address potential duplicates\n",
    "\n",
    "This part aims to address potential duplicates by calculating the similarity between Preprints and Articles\n",
    "\n",
    "- Paper title\n",
    "- Authors\n",
    "\n",
    "After reviewing all the preprints with similarity score higher than 0.5, we found the threshold of 0.6 is a good one for the filtering process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Compute Jaccard similarity between two strings.\"\"\"\n",
    "    str1 = preprocess_text(str1)\n",
    "    str2 = preprocess_text(str2)\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "def find_potential_article(preprint_row, articles_df, threshold=0.5):\n",
    "    \"\"\"Find a potential published article for a given preprint.\"\"\"\n",
    "    preprint_date = preprint_row['publication_date']\n",
    "    preprint_topic = preprint_row['primary_topic.field.display_name']\n",
    "    preprint_title_author = preprint_row['title_author']\n",
    "    \n",
    "    # Filter articles by date and topic\n",
    "    potential_articles = articles_df[\n",
    "        (articles_df['publication_date'] > preprint_date) &\n",
    "        (articles_df['primary_topic.field.display_name'] == preprint_topic)\n",
    "    ]\n",
    "    if potential_articles.empty:\n",
    "        return pd.Series({'matched_article_title_author': None, 'matched_article_date': None, 'similarity': None})\n",
    "    \n",
    "    # Compute similarities and fine the best match\n",
    "    similarities = []\n",
    "    for _, article in potential_articles.iterrows():\n",
    "        sim = jaccard_similarity(preprint_title_author, article['title_author'])\n",
    "        similarities.append((sim, article['title_author'], article['publication_date']))\n",
    "    if similarities:\n",
    "        best_match = max(similarities, key=lambda x: x[0])\n",
    "        if best_match[0] >= threshold:\n",
    "            return pd.Series({\n",
    "                'matched_article_title_author': best_match[1],\n",
    "                'matched_article_date': best_match[2],\n",
    "                'similarity': best_match[0]\n",
    "            })\n",
    "    return pd.Series({'matched_article_title_author': None, 'matched_article_date': None, 'similarity': None})\n",
    "\n",
    "\n",
    "def remove_similar_duplicates(df):\n",
    "    \"\"\"Remove duplicates based on similarity score from a DataFrame.\"\"\"\n",
    "    df['authors'] = df['authorships.raw_author_name'].fillna('').astype(str).str.split('|').apply(lambda x: ', '.join(x) if x else '')\n",
    "    df['title_author'] = df['title'] + ' - ' + df['authors']\n",
    "    dff = df[['doi', 'type', 'title_author', 'publication_date', 'primary_topic.field.display_name']]\n",
    "    dff = dff.sort_values(by='publication_date', ascending=True)\n",
    "    preprints = dff[dff['type'] == 'preprint']\n",
    "    articles = dff[dff['type'] == 'article']\n",
    "    # Apply the function to each preprint\n",
    "    results = []\n",
    "    for _, preprint in tqdm(preprints.iterrows(), total=len(preprints), desc=\"Processing preprints\"):\n",
    "        result = find_potential_article(preprint, articles)\n",
    "        results.append(result)\n",
    "    # Combine results with preprints\n",
    "    preprints_with_matches = pd.concat([preprints.reset_index(drop=True), pd.DataFrame(results)], axis=1)\n",
    "    matched_preprints = preprints_with_matches.dropna(subset=['matched_article_title_author'])\n",
    "    return matched_preprints\n",
    "\n",
    "matched_preprints = remove_similar_duplicates(df2)\n",
    "matched_preprints.to_csv('preprint_article_matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_preprints = pd.read_csv('preprint_article_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69717\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "remove_preprints = matched_preprints[matched_preprints['similarity'] >= 0.6]\n",
    "remove_list = remove_preprints['doi'].tolist()\n",
    "remove_list = [x for x in remove_list if not (isinstance(x, float) and math.isnan(x))]\n",
    "\n",
    "df3 = df2[~df2['doi'].isin(remove_list)]\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check related papers\n",
    "\n",
    "This section aims to verify if a paper aligns with our study scope, as some papers may include the search term but not discuss LLM applications. We check papers in two steps. \n",
    "\n",
    "- Step 1: If the major topic of this paper focuses on natural langauge processing (NLP), artificial intelligence (AI), or any NLP-related tasks, we consider the paper's topic is related.\n",
    "\n",
    "- Step 2: For those papers with the topic not related to any NLP tasks, we use the GPT-4o model to analyze the title and abstract (if available) while also considering the primary topics addressed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/337725868.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['topics.display_name'] = df3['topics.display_name'].astype(str)\n",
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/337725868.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['keywords.display_name'] = df3['keywords.display_name'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42454 27263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/337725868.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df3['possible'] = df3.apply(lambda x: check_related(x['topics.display_name'], x['keywords.display_name']), axis=1)\n",
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/337725868.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['possible'] = df3.apply(lambda x: check_related(x['topics.display_name'], x['keywords.display_name']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "related_topics = ['natural langauge processing', 'language processing', 'artificial intelligence', 'machine learning', \n",
    "                  'deep learning', 'transfer learning', 'data mining', 'text mining', 'speech recognition', 'multimodal',\n",
    "                  'question answering', 'text analysis', 'topic modeling', 'translation', 'sentiment analysis', \n",
    "                  'named entity recognition', 'language analysis']\n",
    "\n",
    "def check_related(topics, keywords):\n",
    "    topics = topics.lower().strip()\n",
    "    keywords = keywords.lower().strip()\n",
    "    if any(rt in topics for rt in related_topics) or any(rt in keywords for rt in related_topics):\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df3['topics.display_name'] = df3['topics.display_name'].astype(str)\n",
    "df3['keywords.display_name'] = df3['keywords.display_name'].astype(str)\n",
    "df3['possible'] = df3.apply(lambda x: check_related(x['topics.display_name'], x['keywords.display_name']), axis=1)\n",
    "df4 = df3[df3['possible'] == 'Yes']\n",
    "df5 = df3[df3['possible'] == 'No'] # These papers are checked by GPT4o\n",
    "print(len(df4), len(df5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/l2jwfcqs4yl3k4j6btbtvjtm0000gr/T/ipykernel_34043/277603897.py:1: DtypeWarning: Columns (23,24,35,52,53,103,106) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df5 = pd.read_csv('check_papers_response.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27263 69717\n"
     ]
    }
   ],
   "source": [
    "df5 = pd.read_csv('check_papers_response.csv')\n",
    "df6 = pd.concat([df4, df5])\n",
    "df6 = df6.drop_duplicates(subset=['title'], keep='last')\n",
    "print(len(df5), len(df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50391\n"
     ]
    }
   ],
   "source": [
    "df6['possible'] = df6['possible'].str.replace('Yes.', 'Yes').replace('No.', 'No')\n",
    "df7 = df6[df6['possible'] == 'Yes']\n",
    "print(len(df7))\n",
    "#df7.to_csv('cleaned_papers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.96        86\n",
      "           1       1.00      0.92      0.96       104\n",
      "\n",
      "    accuracy                           0.96       190\n",
      "   macro avg       0.96      0.96      0.96       190\n",
      "weighted avg       0.96      0.96      0.96       190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "check_df = pd.read_excel('check_papers_samples.xlsx')\n",
    "check_df = check_df.dropna(subset=['Human-annotation'])\n",
    "possible = check_df['possible']\n",
    "human_annotation = check_df['Human-annotation']\n",
    "\n",
    "possible_binary = possible.map({'Yes': 1, 'No': 0})\n",
    "human_annotation_binary = human_annotation.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "report = classification_report(human_annotation_binary, possible_binary)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
